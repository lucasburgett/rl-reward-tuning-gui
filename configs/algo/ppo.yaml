# PPO hyperparameters (works well for both CartPole and LunarLander)
algo_name: ppo

# Core PPO parameters
learning_rate: 3.0e-4
rollout_len: 2048        # n_steps in SB3
num_minibatches: 4       # Used to calculate batch_size = rollout_len // num_minibatches
update_epochs: 4         # n_epochs in SB3
gamma: 0.99
gae_lambda: 0.95
clip_ratio: 0.2          # clip_range in SB3
entropy_coef: 0.0        # ent_coef in SB3
value_coef: 0.5          # vf_coef in SB3
max_grad_norm: 0.5
normalize_adv: true

# Training schedule
checkpoint_every: 10000
eval_every: 20000

# Policy network configuration
policy_kwargs: {}

# Environment-specific presets (can override via CLI)
# For LunarLander: consider n_steps=4096, batch_size=64 for better performance
lunarlander_preset:
  rollout_len: 4096      # Longer rollouts for complex environments
  num_minibatches: 64    # Smaller batch size
  learning_rate: 2.5e-4  # Slightly lower LR
  gamma: 0.999           # Higher discount for sparse rewards