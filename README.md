# Reproducible RL Template

A clean, reproducible template for deep RL experiments with deterministic runs, clear configs, and one-command training/evaluation.

## Features

- ğŸ¯ **Multi-Environment PPO**: Stable-Baselines3 PPO integration with CartPole-v1 and LunarLander-v3
- ğŸ”§ **Reproducible**: Deterministic seeding and configuration management via Hydra
- ğŸ“Š **Advanced Logging**: CSV metrics logging, optional Weights & Biases integration, video recording
- ğŸ—‚ï¸ **Organized Artifacts**: Structured output layout under `artifacts/{env}/{algo}/{seed}/{timestamp}/`
- âš¡ **Fast Setup**: One-command training and evaluation with convenience scripts

## Quick Start

### Install Dependencies
```bash
pip install -r requirements.txt
```

**Note for macOS users**: If you encounter Box2D build issues for LunarLander, install swig:
```bash
brew install swig
```

### Train PPO on CartPole (Fast)
```bash
./scripts/run_cartpole.sh
```

### Train PPO on LunarLander (Day 4 - New!)
```bash
./scripts/run_lunarlander.sh
```

### Manual Training Examples
```bash
# CartPole (100k steps, ~2-3 minutes)
python -m src.train env=cartpole algo=ppo total_steps=100000

# LunarLander-v3 (1M steps, ~30-60 minutes) 
python -m src.train env=lunarlander algo=ppo total_steps=1000000

# LunarLander with optimized hyperparameters
python -m src.train env=lunarlander algo=ppo total_steps=1000000 \
    algo.ppo.rollout_len=4096 algo.ppo.num_minibatches=64

# Enable Weights & Biases logging
python -m src.train env=lunarlander algo=ppo use_wandb=true
```

### Manual Evaluation
```bash
# Evaluate latest checkpoint (auto-finds artifacts)
python -m src.eval env=lunarlander algo=ppo

# Evaluate with video recording
python -m src.eval env=lunarlander algo=ppo eval.record_video=true
```

## Performance Results

The PPO implementation achieves:

### CartPole-v1
- âœ… **500/500 perfect score** consistently 
- âœ… **Fast convergence** in ~20k training steps
- âœ… **Sub-5 minute training** on CPU

### LunarLander-v3 (Day 4)
- âœ… **â‰¥200 mean return** (target achieved with default settings)
- âœ… **Convergence** in 0.5-1.5M training steps (~30-60 minutes)
- âœ… **Deterministic training** with proper seeding across environments

## Configuration

All hyperparameters are configurable via YAML files:

- `configs/algo/ppo.yaml` - PPO hyperparameters with environment presets
- `configs/env/cartpole.yaml` & `configs/env/lunarlander.yaml` - Environment settings  
- `configs/config.yaml` - Training configuration with W&B options
- `configs/eval_config.yaml` - Evaluation configuration

Override any parameter via CLI:
```bash
# Tune hyperparameters
python -m src.train env=lunarlander total_steps=500000 algo.ppo.learning_rate=1e-4

# Enable Weights & Biases with custom project
python -m src.train env=lunarlander use_wandb=true wandb.project=my-rl-experiments
```

## Project Structure

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â””â”€â”€ ppo.py          # PPO agent wrapper (SB3 + CleanRL support)
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ seeding.py      # Deterministic seeding utilities
â”‚   â”‚   â””â”€â”€ logger.py       # CSV logging utilities (Day 4)
â”‚   â”œâ”€â”€ train.py            # Training entrypoint with Hydra
â”‚   â””â”€â”€ eval.py             # Evaluation entrypoint with Hydra  
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ config.yaml         # Main training config with W&B options
â”‚   â”œâ”€â”€ eval_config.yaml    # Evaluation configuration
â”‚   â”œâ”€â”€ algo/ppo.yaml       # PPO hyperparameters + environment presets
â”‚   â””â”€â”€ env/
â”‚       â”œâ”€â”€ cartpole.yaml   # CartPole environment settings
â”‚       â””â”€â”€ lunarlander.yaml # LunarLander environment settings (Day 4)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_cartpole.sh     # CartPole training script
â”‚   â””â”€â”€ run_lunarlander.sh  # LunarLander training script (Day 4)
â”œâ”€â”€ artifacts/              # Organized training outputs (Day 4)
â”‚   â””â”€â”€ {env}/{algo}/{seed}/{timestamp}/
â”‚       â”œâ”€â”€ checkpoints/    # Model checkpoints (.zip files)
â”‚       â”œâ”€â”€ videos/         # Evaluation videos (.mp4 files)
â”‚       â”œâ”€â”€ logs/           # CSV metrics (train_metrics.csv, eval_metrics.csv, monitor.csv)
â”‚       â””â”€â”€ config/         # Saved configurations (train.yaml, eval.yaml)
â””â”€â”€ requirements.txt        # Python dependencies (includes Box2D)
```

## Day 4 - LunarLander + Advanced Logging

Day 4 adds LunarLander-v3 support with enhanced logging and artifacts management:

### Key Day 4 Features
- ğŸŒ™ **LunarLander-v3** environment with Box2D physics
- ğŸ“Š **CSV Logging**: Training/eval metrics saved to structured CSV files
- ğŸª **Weights & Biases**: Optional W&B integration for experiment tracking  
- ğŸ—‚ï¸ **Artifacts Layout**: Organized structure under `artifacts/{env}/{algo}/{seed}/{timestamp}/`
- ğŸ“¹ **Enhanced Video Recording**: Automatic video capture during evaluations
- âš¡ **Training Optimizations**: Environment-specific hyperparameter presets

### LunarLander Performance Tips
- **Target score**: â‰¥200 mean return (typically achieved in 500k-1.5M steps)
- **Recommended hyperparameters**: Use `algo.ppo.rollout_len=4096` and `algo.ppo.num_minibatches=64` for better performance
- **Training time**: ~30-60 minutes on modern CPU, ~10-20 minutes on GPU
- **Box2D installation**: May require `brew install swig` on macOS

### Weights & Biases Setup
```bash
# Install wandb (optional)
pip install wandb

# Enable W&B logging
python -m src.train env=lunarlander use_wandb=true wandb.project=my-experiments

# Configure W&B settings in configs/config.yaml:
# wandb:
#   project: rl-template
#   entity: your-wandb-username  
#   group: ${env}-${algo}
```

## Troubleshooting

- **Box2D build errors**: Install swig first: `brew install swig` (macOS) or `apt install swig` (Ubuntu)
- **ffmpeg not found**: Install ffmpeg for video recording
  - macOS: `brew install ffmpeg`
  - Ubuntu: `apt install ffmpeg`
- **Memory issues**: Reduce `total_steps` or use `device=cpu`
- **LunarLander not converging**: Increase `total_steps` to 1M+ or try the optimized hyperparameters
- **Missing dependencies**: Run `pip install -r requirements.txt`
- **Config errors**: Ensure you're using the root `configs/` directory structure
- **Artifacts not found**: Check the `artifacts/` directory structure matches `{env}/{algo}/{seed}/{timestamp}/`

## Development Notes

This template follows the patterns defined in `CLAUDE.md`:
- Type-hinted Python with Black formatting
- Modular design with clear separation of concerns  
- Hydra configuration management
- Comprehensive error handling and logging
- Deterministic training for reproducible results
