# Reproducible RL Template

A clean, reproducible template for deep RL experiments with deterministic runs, clear configs, and one-command training/evaluation.

## Features

- ðŸŽ¯ **PPO Implementation**: Stable-Baselines3 PPO integration with CartPole-v1 achieving â‰¥475/500 score
- ðŸ”§ **Reproducible**: Deterministic seeding and configuration management via Hydra
- ðŸ“Š **Monitoring**: Automatic checkpointing, evaluation, and video recording
- âš¡ **Fast Setup**: One-command training and evaluation with convenience scripts

## Quick Start

### Install Dependencies
```bash
pip install -r requirements.txt
```

### Train PPO on CartPole (Recommended)
```bash
./scripts/run_cartpole.sh
```

### Manual Training
```bash
python -m src.train env=cartpole algo=ppo total_steps=100000
```

### Manual Evaluation
```bash
python -m src.eval env=cartpole algo=ppo log_dir=experiments/YYYY-MM-DD/HH-MM-SS
```

## Performance Results

The PPO implementation achieves:
- âœ… **500/500 perfect score** on CartPole-v1 
- âœ… **Fast convergence** in ~20k training steps
- âœ… **Deterministic training** with proper seeding
- âœ… **Sub-5 minute training** on CPU

## Configuration

All hyperparameters are configurable via YAML files:

- `configs/algo/ppo.yaml` - PPO hyperparameters
- `configs/env/cartpole.yaml` - Environment settings  
- `configs/config.yaml` - Training configuration
- `configs/eval_config.yaml` - Evaluation configuration

Override any parameter via CLI:
```bash
python -m src.train total_steps=200000 algo.learning_rate=1e-4
```

## Project Structure

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â””â”€â”€ ppo.py          # PPO agent wrapper (SB3 + CleanRL support)
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ seeding.py      # Deterministic seeding utilities
â”‚   â”œâ”€â”€ train.py            # Training entrypoint with Hydra
â”‚   â””â”€â”€ eval.py             # Evaluation entrypoint with Hydra
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ config.yaml         # Main training configuration
â”‚   â”œâ”€â”€ eval_config.yaml    # Evaluation configuration
â”‚   â”œâ”€â”€ algo/ppo.yaml       # PPO hyperparameters
â”‚   â””â”€â”€ env/cartpole.yaml   # Environment settings
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_cartpole.sh     # Convenience training script
â”œâ”€â”€ experiments/            # Training outputs (auto-generated)
â”‚   â””â”€â”€ YYYY-MM-DD/HH-MM-SS/
â”‚       â”œâ”€â”€ checkpoints/    # Model checkpoints
â”‚       â””â”€â”€ videos/         # Evaluation videos
â””â”€â”€ requirements.txt        # Python dependencies
```

## Troubleshooting

- **ffmpeg not found**: Install ffmpeg for video recording
  - macOS: `brew install ffmpeg`
  - Ubuntu: `apt install ffmpeg`
- **Memory issues**: Reduce `total_steps` or use `device=cpu`
- **Poor performance**: Increase `total_steps` to 200k+ or tune hyperparameters
- **Missing dependencies**: Run `pip install -r requirements.txt`
- **Config errors**: Ensure you're using the root `configs/` directory structure

## Development Notes

This template follows the patterns defined in `CLAUDE.md`:
- Type-hinted Python with Black formatting
- Modular design with clear separation of concerns  
- Hydra configuration management
- Comprehensive error handling and logging
- Deterministic training for reproducible results
